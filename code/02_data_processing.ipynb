{
 "cells": [
  {
   "cell_type": "raw",
   "id": "310f7d18",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db549a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A note on regridding\n",
    "    # We use the XESMF package to regrid data to a 1x1 degree global grid\n",
    "    # From their documentation, \"When dealing with global grids, we need to set periodic=True, otherwise data along the meridian line will be missing\"\n",
    "    # \"For real-world data, it is generally recommended to use conservative for [high resolution to low resolution],\n",
    "        # because it takes average over small source grid boxes, while bilinear and nearest_s2d effectively throw away most of source grid boxes.\" \n",
    "        # More at https://xesmf.readthedocs.io/en/latest/notebooks/Compare_algorithms.html\n",
    "    # We follow this except for ERA5 SLP which we use bilinear. Using conservative and conservative_normed gives a warning \"Latitude is outside of [-90, 90]\" and we have a separate coastal filling process. \n",
    "        # See https://xesmf.readthedocs.io/en/latest/notebooks/Masking.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2fca94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import datetime\n",
    "import numpy.ma as ma\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import os\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10e2558",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This file contains configuration details like API keys and passwords\n",
    "global_vars = yaml.safe_load(open('../config.yml', 'r') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b5c09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if running locally; some issues with xesmf versioning so need this:\n",
    "ESMFMKFILE = global_vars['ESMFMKFILE']\n",
    "%env ESMFMKFILE $ESMFMKFILE\n",
    "import xesmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53354db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This has custom functions - several but the fCO2_to_pCO2 function is key\n",
    "%run ./00_custom_functions.ipynb\n",
    "#fCO2_to_pCO2(380, 8) #381.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d540c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To ingest the raw data downloaded in the previous script, we need to point to the location of the downloads\n",
    "# Data will be read at this location in folders such as root/SST/originals/ and root/SSS/orginals/\n",
    "# Processed data will be saved to folders such as root/SST/processed/ \n",
    "# Note that data file names within these folders have been hardcoded in each section below\n",
    "cloud = False\n",
    "if cloud:\n",
    "    data_folder_root = global_vars['download_folder_cloud']\n",
    "else:\n",
    "    data_folder_root = global_vars['download_folder_local']\n",
    "print(data_folder_root)\n",
    "\n",
    "\n",
    "# The following two variables are used to slice the data to desired time frames for consistency or to backfill historical data will averages.\n",
    "# Given the time range limitiations of the raw data, this primarily affects MLD (1 year repeated) and CHL (linear interpoloation)\n",
    "processed_start_yearmonth = '1982-01'  \n",
    "processed_end_yearmonth = '2023-04'   \n",
    "figsizew, figsizeh = 6, 3  #figure size for maps (width/height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f5217c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create desired resolution and time frame\n",
    "ylat = xr.DataArray(data=[x+.5 for x in range(-90, 90, 1)], dims=['ylat'], coords=dict( ylat=(['ylat'],[x+.5 for x in range(-90, 90, 1)]) ),)\n",
    "xlon = xr.DataArray(data=[x+.5 for x in range(-180,180,1)], dims=['xlon'], coords=dict( xlon=(['xlon'],[x+.5 for x in range(-180,180,1)]) ),)\n",
    "ttime = pd.date_range(start=str(processed_start_yearmonth), end=str(processed_end_yearmonth),freq='MS') + np.timedelta64(14, 'D') #time should be monthly on the middle of the month\n",
    "        #note that the time doesnt affect regridding but we do use this time to overwrite the monthly dates so its consistent\n",
    "\n",
    "ideal_grid = xr.Dataset({'time':(['time'],ttime.values), 'latitude':(['latitude'],ylat.values),'longitude':(['longitude'],xlon.values)}) #must be named this way for old XESFM versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d654590",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a566072",
   "metadata": {},
   "source": [
    "## Temperature (SST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82501bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sst = xr.open_dataset(data_folder_root+r'SST/originals/SST_NOAA_OI-V2-HighRes_198109-202306.nc')  \n",
    "#sst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802025fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sst_filtered = sst.sel(time=slice(str(processed_start_yearmonth),str(processed_end_yearmonth))) #already monthly so select desired years\n",
    "sst_regridder = xesmf.Regridder(sst_filtered, ideal_grid, 'conservative', periodic=True)  #see notes above on why conservative over bilinear\n",
    "sst_out = sst_regridder(sst_filtered, keep_attrs=True)\n",
    "#sst_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f381e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_out = sst_out.rename({'latitude': 'ylat','longitude': 'xlon'}) #rename to be consistent with prior work\n",
    "sst_out = sst_out.assign_coords(time=ttime) #overwrite time dimension to be midmonth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00042c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_netcdf_with_date(sst_out, data_folder_root+'SST/processed/', 'SST_NOAA_OI-V2-1x1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0850a3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(figsizew,figsizeh))\n",
    "plt.contourf(ideal_grid.longitude,ideal_grid.latitude,sst_out.sst[-1,:,:]) #just choosing latest time slice\n",
    "plt.title('SST (latest month)'); plt.xlabel('Lon'); plt.ylabel('Lat')\n",
    "plt.colorbar().set_label('deg C');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b475ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78894572",
   "metadata": {},
   "source": [
    "## Salinity (SSS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d469c531",
   "metadata": {},
   "outputs": [],
   "source": [
    "sss = xr.open_mfdataset(data_folder_root+'SSS/originals/EN.4.2.2.f.analysis.g10.*.nc') \n",
    "#sss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb0879e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#effectively ignore the depth dimension because we dont need it\n",
    "sss_nodepth = xr.Dataset({'sss':(['time','lat','lon'],sss.salinity[:,0,:,:].values),'time':(['time'],sss.time.data),'lat':(['lat'],sss.lat.data),'lon':(['lon'],sss.lon.data)})\n",
    "#Note this removes attributes from SSS raw data so we lose some metadata but this is OK for this analysis\n",
    "sss_filtered = sss_nodepth.sel(time=slice(str(processed_start_yearmonth),str(processed_end_yearmonth))) #filter years\n",
    "\n",
    "#Regrid lat/lon\n",
    "sss_regridder = xesmf.Regridder(sss_filtered, ideal_grid, 'bilinear', periodic=True)  #data is already 1x1 but we want to shift coordinates\n",
    "sss_out = sss_regridder(sss_filtered, keep_attrs=True)\n",
    "#sss_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3065324b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sss_out = sss_out.rename({'latitude': 'ylat','longitude': 'xlon'}) #rename to be consistent with prior work\n",
    "sss_out = sss_out.assign_coords(time=ttime) #overwrite time dimension to be midmonth\n",
    "#sss_out = sss_out.assign_coords(time=ttime[:-1]) #you can use this line if SSS hasnt updated and we are missing the last month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4064d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_netcdf_with_date(sss_out, data_folder_root+'SSS/processed/', 'SSS_Met-Office-Hadley-Centre_EN422f-g10-analyses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4021d932",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(figsizew,figsizeh))\n",
    "plt.contourf(ideal_grid.longitude,ideal_grid.latitude,sss_out.sss[-1,:,:]) #just choosing latest time slice\n",
    "plt.title('SSS (latest month)'); plt.xlabel('Lon'); plt.ylabel('Lat')\n",
    "plt.colorbar().set_label('g/kg');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c248c3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43e01a86",
   "metadata": {},
   "source": [
    "## Mixed Layer Depth (MLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac7738c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mld_ds = xr.open_dataset(data_folder_root+r'MLD/originals/MLD_IFREMER-deBoyer_DT02-c1m_2008.nc', decode_times=False)\n",
    "mld = mld_ds.mld  #just need the mld variable. Data is 2x2 resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb3d78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#there are strange anomalies in the raw data which we are going to filter out\n",
    "mld_filtered = mld.where(((mld>0)&(mld<5000))) \n",
    "\n",
    "#TODO - Document these steps\n",
    "mld_filtered2 = np.where(np.isnan(mld_filtered), ma.array(mld_filtered, mask=np.isnan(mld_filtered)).mean(axis=2)[:,:,np.newaxis], mld_filtered)\n",
    "mld_filtered2 = np.where(mld_filtered2==0,np.nan,mld_filtered2)\n",
    "mld_filtered2 = mld_filtered2 #[0:12,:,:] only 1 year needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7ce20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mld_in = xr.Dataset({'mld':(['time','lat','lon'],mld_filtered2),'time':(['time'],range(0,12)),'lat':(['lat'],mld_filtered.lat.values),'lon':(['lon'],mld_filtered.lon.values)})\n",
    "\n",
    "mld_regridder = xesmf.Regridder(mld_in, ideal_grid, 'bilinear', periodic=True) \n",
    "mld_out = mld_regridder(mld_filtered2, keep_attrs=True)\n",
    "mld_out = np.where(np.isnan(mld_out),0,mld_out)\n",
    "#mld_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15676287",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we now have 12 months of MLD data, let's replicate for the desired time frame so merging is easier later\n",
    "mld_out\n",
    "mld_out_full = np.empty(shape=(len(ttime),180,360)) \n",
    "#now fill array with averaged year\n",
    "for i, m in enumerate(ttime): \n",
    "    mld_out_full[i,:,:] = mld_out[m.month-1,:,:]\n",
    "#mld_out_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0987d611",
   "metadata": {},
   "outputs": [],
   "source": [
    "mld_out_full_xr = xr.Dataset({'mld':([\"time\",\"ylat\",\"xlon\"],mld_out_full.data)},\n",
    "                        coords={'time': (['time'],ttime.values),'ylat':(['ylat'],ideal_grid.latitude.data),'xlon':(['xlon'],ideal_grid.longitude.data)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8893f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_netcdf_with_date(mld_out_full_xr, data_folder_root+'MLD/processed/', 'MLD_IFREMER-deBoyer_DT02-c1m-1x1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9d23ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "levelspace = np.linspace(0,560,11)\n",
    "fig,ax = plt.subplots(1,2,figsize=(figsizew*2, figsizeh))\n",
    "x0=ax[0].contourf(ideal_grid.longitude,ideal_grid.latitude,mld_out_full_xr.mld[6,:,:], levels=levelspace) #just choosing a summer month (July)\n",
    "x1=ax[1].contourf(ideal_grid.longitude,ideal_grid.latitude,mld_out_full_xr.mld[0,:,:], levels=levelspace) #just choosing a winter month (Jan)\n",
    "ax[0].set_title(\"MLD July\"); ax[1].set_title(\"MLD Jan\");\n",
    "ax[0].set_xlabel('Lon'); ax[0].set_ylabel('Lat');\n",
    "ax[1].set_xlabel('Lon'); ax[1].set_ylabel('Lat');\n",
    "plt.colorbar(x0, ax=ax[0]).set_label('meters');\n",
    "plt.colorbar(x1, ax=ax[1]).set_label('meters');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a03322d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aeaa45cc",
   "metadata": {},
   "source": [
    "## Chlorophyll (CHL)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2d88b632",
   "metadata": {},
   "source": [
    "#According to the Bennington paper, we fill the missing winter months at the poles by linearly interpolating between the last month observed prior to the winter and the first month observed after winter. Also, for prior to 1998, we use the climatology of Chlorophyll-a calculated from observations we do have.\n",
    "\n",
    "#General Process: \n",
    "#0) combine data and filter time to relevant period\n",
    "#1) group by month to get an 'average' year\n",
    "#2) loop that year into two years so that we can linearly interpolate without boundary issues in Jan/Dec. Only need the middle 12 months.\n",
    "#3) fill in 1982 to 1997 with that averaged year\n",
    "#4) combine with the 1998+ data to get full set and interpolate on full set again to fill missing\n",
    "#5) fill remaining missing with the averaged year \n",
    "#6) regrid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee305941",
   "metadata": {},
   "outputs": [],
   "source": [
    "#0) First combine data and filter for the relevant time period. Because CHL data does not have a time dimension in raw data, we need to add it to each file before/during concatenation \n",
    "files = glob.glob(data_folder_root+'CHL/originals/*.nc')\n",
    "chl = xr.concat([add_time_to_globcolour(fl) for fl in files], dim='time')\n",
    "#chl\n",
    "chl_filter = chl.sel(time=slice(str(processed_start_yearmonth),str(processed_end_yearmonth))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56cae84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1) Next we group by month to get and 'average' year\n",
    "    #Note that we are using data from the desired time frame to calculate an average year. This means past data will change when more years are added.\n",
    "chl_avg_by_month = chl_filter.CHL1_mean.groupby(\"time.month\").mean(\"time\")  #only need the mean variable\n",
    "#2) Then we loop that year (duplicate) into two years so that we can linearly interpolate without boundary issues in Jan/Dec. We will only need the middle 12 months.\n",
    "chl_looped = np.empty(shape=(24,180,360))  #2 years of data.\n",
    "chl_looped[0:6,:,:] = chl_avg_by_month[6:12,:,:]  #set start of loop to be July to Dec. Winter at the north pole is at the end/start of the year\n",
    "chl_looped[6:18,:,:] = chl_avg_by_month\n",
    "chl_looped[18:24,:,:] = chl_avg_by_month[0:6,:,:]  #set end of loop to be Jan to Jun\n",
    "chl_looped_xr = xr.Dataset({'chl':(['time','lat','lon'],chl_looped)},\n",
    "                       coords={'time':(['time'],range(0,24)),'lat':(['lat'],chl_avg_by_month.lat.values),'lon':(['lon'],chl_avg_by_month.lon.values)})\n",
    "chl_looped_interpolated = chl_looped_xr.chl.interpolate_na(dim='time',method='linear',limit=7)\n",
    "chl_avg_year = chl_looped_interpolated[6:18,:,:]   #the full year we interpolated is in the middle. Starts in Jan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f7c946",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3) Now we fill in 1982 to 1997 with that averaged year\n",
    "earliest_chl_date = chl.time.min().data.astype('datetime64[s]').item()\n",
    "chl_missing_years = np.empty(shape=(sum(ttime < earliest_chl_date),180,360))  #create empty array of size equal to number of missing months\n",
    "chl_missing_years = xr.DataArray(chl_missing_years, coords=dict(time=ttime[ttime < earliest_chl_date], lat=chl_avg_year.lat, lon=chl_avg_year.lon), dims=[\"time\", \"lat\", \"lon\"]) #make it an xr\n",
    "chl_avg_fullset = np.empty(shape=(len(ttime),180,360))   #this is just the average which we will use later to fill missing holes from interpolation\n",
    "#now fill array with averaged year\n",
    "for i, m in enumerate(ttime):   #loop through months between start and end date\n",
    "    chl_avg_fullset[i,:,:] = chl_avg_year[m.month-1,:,:]\n",
    "    if m < earliest_chl_date: \n",
    "        chl_missing_years[i,:,:] = chl_avg_year[m.month-1,:,:]\n",
    "#chl_missing_years.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdabcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4) Next, combine the pre-1997 set with the 1998+ data to get a full set. Then interpolate on full set again to fill missing points.\n",
    "chl_filter_with_missing = xr.concat([chl_missing_years, chl_filter.CHL1_mean], dim='time')\n",
    "chl_filter_interpolate = chl_filter_with_missing.interpolate_na(dim='time',method='linear',limit=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335e8dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5) For any remaining missing values that could not be interpolated, we fill with the averaged climatology\n",
    "chl_filter_interpolate_xr = xr.Dataset({'chl':(['time','lat','lon'],chl_filter_interpolate.data)  #make as a dataset so we can use fillna\n",
    "                                       #,'clim_repeat':(['time','lat','lon'], chl_avg_fullset)    #This field was used in a prior version of code but is not required\n",
    "                            }, coords={'time':(['time'],chl_filter_interpolate.time.values),'lat':(['lat'],chl_filter_interpolate.lat.values),'lon':(['lon'],chl_filter_interpolate.lon.values)})\n",
    "chl_filter_final = chl_filter_interpolate_xr.fillna(chl_avg_fullset) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f71fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6) Regrid \n",
    "chl_regridder = xesmf.Regridder(chl_filter_final, ideal_grid, 'bilinear', periodic=True)\n",
    "chl_out = chl_regridder(chl_filter_final, keep_attrs=True)\n",
    "chl_out = chl_out.rename({'latitude': 'ylat','longitude': 'xlon'}) \n",
    "chl_out['chl'].attrs['description'] = \"Interpolated linearly between months, climatology prior to 1998-01\"  \n",
    "chl_out['chl'].attrs['units'] = \"mg / m3\"\n",
    "#chl_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d79a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_netcdf_with_date(chl_out, data_folder_root+'CHL/processed/', 'CHL_ARI-ST-GlobColour_L3m-GLOB-100-merged-GSM-CHL1', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417eb4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(figsizew,figsizeh))\n",
    "plt.contourf(ideal_grid.longitude,ideal_grid.latitude,np.ma.log10(chl_out.chl[12*20+6,:,:]))  #just one summer month; log scale to make more readable\n",
    "plt.title('CHL (July)'); plt.xlabel('Lon'); plt.ylabel('Lat')\n",
    "plt.colorbar().set_label('mg/m3 log scale');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53123e6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8da4ecf2",
   "metadata": {},
   "source": [
    "## Sea Level Pressure (SLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13a8c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "era5 = xr.open_mfdataset(data_folder_root+r'SLP/originals/SLP_ECMWF_ERA5-monthly-reanalysis-MSLP_*.nc') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0d5741",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Regrid era5\n",
    "mslp_highres = era5.sel(expver=1).msl/100  #do do not need the experimental/recent data because socat is only up till end of the prior yer. Also Pa to HPa. \n",
    "    #Note the expver variable in era5 indicates new experimental data: #https://confluence.ecmwf.int/pages/viewpage.action?pageId=173385064\n",
    "mslp_highres = xr.Dataset({'mslp':(['time','latitude','longitude'],mslp_highres.data),'time':(['time'],mslp_highres.time.data),'latitude':(['latitude'],mslp_highres.latitude.data),'longitude':(['longitude'],mslp_highres.longitude.data)})\n",
    "mslp_highres_regridder = xesmf.Regridder(mslp_highres, ideal_grid, 'bilinear', periodic=True) #See note above about bilinear vs conservative\n",
    "mslp_out = mslp_highres_regridder(mslp_highres, keep_attrs=False)\n",
    "#mslp_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec80bcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mslp_out.mslp.attrs['units'] = 'hPa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce140348",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_netcdf_with_date(mslp_out, data_folder_root+'SLP/processed/', 'SLP_ECMWF_ERA5-monthly-reanalysis-1x1-MSLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23101b55",
   "metadata": {},
   "source": [
    "## pCO2"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6ecec40c",
   "metadata": {},
   "source": [
    "General Process:\n",
    "1. Ingest SLP and fCO2\n",
    "2. Regrid and align to time frame\n",
    "3. Calculate pCO2 from these variables\n",
    "4. Compile new xarray dataset and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e9d7c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "socat = xr.open_dataset(data_folder_root+r'pCO2/originals/fCO2_SOCOVV_SOCAT-gridded-monthly_2022.nc') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7db8aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get relevant variables and filter for desired years \n",
    "co2_processed_end_yearmonth = socat.tmnth.max().data.astype('datetime64[s]').item().strftime('%Y-%m') #Note that fCO2 is only up til 2 years ago if (released every June)\n",
    "fco2 = socat.fco2_ave_weighted\n",
    "fco2 = fco2.sel(tmnth=slice(str(processed_start_yearmonth),str(co2_processed_end_yearmonth))) \n",
    "fco2_sst = socat.sst_ave_weighted\n",
    "fco2_sst = fco2_sst.sel(tmnth=slice(str(processed_start_yearmonth),str(co2_processed_end_yearmonth)))\n",
    "fco2_mslp = mslp_out.mslp\n",
    "fco2_mslp = fco2_mslp.sel(time=slice(str(processed_start_yearmonth),str(co2_processed_end_yearmonth)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf3dd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pco2 = fCO2_to_pCO2(fco2, fco2_sst, fco2_mslp, tempEQ_C=None)\n",
    "pco2 = np.where((pco2>200)&(pco2<650),pco2,np.nan) #where socat pco2 values are <200 and >650 we are changing to nans as a source of quality control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e6508e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Realign time to our grid for each available month (ex shift from 16th to 15th for consistency)\n",
    "fco2_ttime = ttime[pd.DataFrame(ttime,columns=['t']).t.between(fco2.tmnth.min().data.astype('datetime64[s]').item().strftime('%Y-%m'), fco2.tmnth.max().data.astype('datetime64[s]').item(), inclusive='both')]\n",
    "    #Alternatively can use this: fco2_ttime = pd.date_range(start=fco2.tmnth.min().item(), end=fco2.tmnth.max().item(),freq='MS') + np.timedelta64(14, 'D')\n",
    "#fco2_ttime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c479775d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pco2_out = xr.Dataset(  {\n",
    "                        'pco2':([\"time\",\"ylat\",\"xlon\"],pco2.data),\n",
    "                        #'socat_mask':([\"time\",\"ylat\",\"xlon\"],np.where(~np.isnan(pco2),1,np.nan)),  #Prior code included additional variables which are not needed\n",
    "                        #'socat_sst':([\"time\",\"ylat\",\"xlon\"],fco2_sst.data),\n",
    "                        #'mslp':([\"time\",\"ylat\",\"xlon\"],fco2_mslp.data),\n",
    "                        'fco2':([\"time\",\"ylat\",\"xlon\"],fco2.data)\n",
    "                        },\n",
    "                        coords={'time': (['time'],fco2_ttime.values),\n",
    "                          'ylat': (['ylat'],fco2.ylat.data),\n",
    "                          'xlon': (['xlon'],fco2.xlon.data)}\n",
    "                    )\n",
    "pco2_out.pco2.attrs['units'] = 'uatm'\n",
    "pco2_out.pco2.attrs['description'] = 'uses fco2 (weighted), sst (weighted) and MSLP to convert to pCO2 via fCO2_to_pCO2 script'\n",
    "pco2_out.pco2.attrs['description2'] = 'socat pco2 values <200 and >650 have been changed to nans as a source of quality control' \n",
    "#pco2_out.socat_sst.attrs['units'] = 'deg C'\n",
    "#pco2_out.mslp.attrs['units'] = 'hPa'\n",
    "pco2_out.fco2.attrs['units'] = 'uatm'\n",
    "#pco2_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b5410a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output_netcdf_with_date(pco2_out, data_folder_root+'pCO2/processed/', 'pCO2_LEAP_SOCAT-ERA5-weighted', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6df0a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(figsizew,figsizeh))\n",
    "plt.contourf(ideal_grid.longitude,ideal_grid.latitude,np.nanmax(pco2_out.pco2, axis=0)) \n",
    "plt.title('Max pCO2 (across time)'); plt.xlabel('Lon'); plt.ylabel('Lat')\n",
    "plt.colorbar().set_label('uatm');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e7342a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a81b979a",
   "metadata": {},
   "source": [
    "# Atmospheric CO2 (xCO2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa0060d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data was downloaded monthly as csv. Convert to a xarray\n",
    "#Note that the number of header rows (55) is hardcoded here\n",
    "xco2_df = pd.read_csv(data_folder_root+r'xCO2/originals/xCO2_NOAA_xCO2-mm-gl-monthly_197901-202306.csv', skiprows=55)  \n",
    "#xco2_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c53fb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "xco2_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed15fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make time the index; drop unneeded columns; convert to xarray\n",
    "xco2_df['time'] = xco2_df.apply(lambda row: datetime.datetime(int(row.year), int(row.month), 15), axis=1) #putting as 15th of the month\n",
    "xco2_df['time'] = pd.to_datetime(xco2_df.time)\n",
    "xco2_df.index = pd.DatetimeIndex(xco2_df.time)\n",
    "\n",
    "xco2_xr = xco2_df[['trend']].to_xarray() #we only want the \"trend\" field because it matches historical work that did not factor seasonality \n",
    "xco2_xr_filtered = xco2_xr.sel(time=slice(str(processed_start_yearmonth),str(processed_end_yearmonth))) #filter for desired time\n",
    "xco2_xr_filtered = xco2_xr_filtered.rename({'trend': 'xco2_trend'})\n",
    "#xco2_xr_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1010393f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_netcdf_with_date(xco2_xr_filtered, data_folder_root+'xCO2/processed/', 'xCO2_NOAA_xCO2-mm-gl-monthly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44f72de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(figsizew,figsizeh))\n",
    "plt.plot(xco2_xr_filtered.time,xco2_xr_filtered.xco2_trend)\n",
    "plt.title('Atmospheric CO2 (trend)'); plt.xlabel('Year'); plt.ylabel('xCO2 (uatm)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3186654",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leap_co2_v1",
   "language": "python",
   "name": "leap_co2_v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
