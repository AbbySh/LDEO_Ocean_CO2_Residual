{
 "cells": [
  {
   "cell_type": "raw",
   "id": "4aeaadc9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1626579d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "import yaml\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import cmocean as cm    \n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import r2_score, max_error, mean_squared_error, mean_absolute_error, median_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca7acf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This file contains configuration details like API keys and passwords\n",
    "global_vars = yaml.safe_load(open('../config.yml', 'r') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f167a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This has custom functions - model evaluation and graphing\n",
    "%run ./00_custom_functions.ipynb   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adf97d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set base folders\n",
    "cloud = False\n",
    "if cloud:\n",
    "    input_folder_root = global_vars['reconstruction_folder_cloud']\n",
    "else:\n",
    "    input_folder_root = global_vars['reconstruction_folder_local']\n",
    "output_folder = input_folder_root #Because we are inputting and outputting processed files, the folder can be the same\n",
    "\n",
    "full_dataset_file = input_folder_root + 'pCO2_LEAP_fco2-residual-full-dataset-preML_198201-202304.nc' #name of file from previous script\n",
    "print(full_dataset_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6362b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is where we set parameters for the algorithm\n",
    "\n",
    "#The following two variables will cut the data to use for training (and prediction)\n",
    "ml_start_yearmonth = '1982-01' \n",
    "ml_end_yearmonth = '2022-12'   \n",
    "\n",
    "#This next variable sets how many models will be trained on different sets of months\n",
    "#When number_of_runs = 5, the first model is trained on months 1-4 and evaluated on month 5. The next model on 2-5 and evaluated on 1. And so forth. \n",
    "number_of_runs = 5\n",
    "\n",
    "#Next set the random seeds used for training (should match the number of runs)\n",
    "if number_of_runs == 5:\n",
    "    random_seeds = np.array([47,16,2,31,91])  #These are from historical work\n",
    "else:\n",
    "    random_seeds = np.random.randint(1,100,number_of_runs)  #or any set of numbers desired\n",
    "print(f'Random Seeds {random_seeds}')\n",
    "\n",
    "#The next variable is for the XGBoost method. These hyperparameters were determined via a grid search in previous iterations. \n",
    "best_params = {'max_depth': 9, 'n_estimators': 1000} \n",
    "jobs = -1         #Number of cores you have access to for model training; -1 for all available ones\n",
    "\n",
    "#This variable is a list of features used for pCO2 Residual machine learning\n",
    "feature_sel = ['sst','sst_anomaly','sss','sss_anomaly','chl_log','chl_log_anomaly','mld_log','xco2_trend','A','B','C','T0','T1']\n",
    "target_sel = ['fco2_nonT']\n",
    "for_final_sel = ['fco2','fco2_T'] #needed for model evaulation on reconstructed pco2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1835bb34",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c95ab97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#These variables are based on the variables set at the top and are used to divide into train/test sets. No changes needed\n",
    "ttime = pd.date_range(start=str(ml_start_yearmonth), end=str(ml_end_yearmonth),freq='MS') + np.timedelta64(14, 'D') \n",
    "month_run_number = []\n",
    "for i, m in enumerate(ttime): month_run_number.append((i % number_of_runs) + 1)\n",
    "month_traintest = xr.Dataset({'month_traintest':(['time'], month_run_number)}, coords={'time':(['time'],ttime)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdc50b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the main dataframe created in the prior script\n",
    "xrfull = xr.open_dataset(full_dataset_file)\n",
    "\n",
    "xr_for_ml = xrfull.get(feature_sel + target_sel + for_final_sel) #variables needed for ML plus for getting back to pco2 from nonT\n",
    "xr_for_ml = xr_for_ml.sel(time=slice(str(ml_start_yearmonth),str(ml_end_yearmonth)))  #filter for time frame we want\n",
    "xr_for_ml = xr_for_ml.merge(month_traintest, compat='identical')   #add variable to determine train vs test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d630a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = xr_for_ml.to_dataframe() #expensive function; need lots of RAM\n",
    "#df.head(5)\n",
    "#df.describe()\n",
    "df_for_ml = df[(~df.isna().any(axis=1))]  #only keep points that are not null. Note that the target_sel has same number of points as the for_final_sel variables \n",
    "print(f'Number of points in time/space for training: {df_for_ml.shape[0]}')\n",
    "#df_for_ml\n",
    "\n",
    "df_for_recon = df.loc[:,feature_sel]\n",
    "df_for_recon = df_for_recon[(~df_for_recon.isna().any(axis=1))]  #only predict on points with all variables globally\n",
    "print(f'Number of points in time/space available to reconstruct: {df_for_recon.shape[0]}')\n",
    "#df_for_recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2572691e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_evaluation_ocean_co2_nonT = []\n",
    "model_evaluation_ocean_co2 = []\n",
    "ocean_co2_nonT_recon_list = np.empty([df_for_recon.shape[0], number_of_runs])  #placeholder np array for reconstructed data\n",
    "\n",
    "for run in range(1,number_of_runs+1):\n",
    "\n",
    "    X_train = df_for_ml[df_for_ml.month_traintest != run].loc[:,feature_sel]#.to_numpy()\n",
    "    y_train = df_for_ml[df_for_ml.month_traintest != run].loc[:,target_sel]#.to_numpy().ravel()\n",
    "    X_test = df_for_ml[df_for_ml.month_traintest == run].loc[:,feature_sel]#.to_numpy()\n",
    "    y_test = df_for_ml[df_for_ml.month_traintest == run].loc[:,target_sel]#.to_numpy().ravel()\n",
    "    #y_ocean_po2_test = df_for_ml[df_for_ml.month_traintest == run].loc[:,for_final_sel]  #The variables we need to reconstruct pco2 from pco2_nonT in the test set\n",
    "\n",
    "    model = XGBRegressor(random_state=random_seeds[run-1], **best_params, n_jobs=jobs)\n",
    "    print(f'----------Training Run {run} on '+datetime.datetime.now().strftime('%Y-%m-%d %H:%M')+'...----------')\n",
    "    model.fit(X_train, y_train) \n",
    "    \n",
    "    ############# Evaluation\n",
    "    # Calculate some test error metrics and store in a dictionary\n",
    "    y_pred_test = model.predict(X_test)\n",
    "\n",
    "    print('-------Performance on ocean_co2_nonT-------')\n",
    "    test_performance = evaluate_test(y_test.to_numpy().ravel(), y_pred_test)\n",
    "    model_evaluation_ocean_co2_nonT.append(test_performance)\n",
    "    print(test_performance)\n",
    "\n",
    "    #print('-------Performance on ocean_co2 overall-------')\n",
    "    #y_final = y_ocean_co2_test[target_sel].to_numpy().ravel()   # Real pCO2 from SOCAT (for test set)\n",
    "    #y_ocean_co2t = y_ocean_co2_test['fpco2'+'_T'].to_numpy().ravel() # pCO2_T (already calculated)\n",
    "    #y_pred_final = y_pred_test + y_ocean_co2t\n",
    "    #ocean_co2_performance = evaluate_test(y_final, y_pred_final)\n",
    "    #model_evaluation_ocean_co2.append(ocean_co2_performance)\n",
    "    #print(ocean_co2_performance)\n",
    "    \n",
    "    ############# Reconstruction\n",
    "    print(f'----------Reconstructing Globally Run {run} on '+datetime.datetime.now().strftime('%Y-%m-%d %H:%M')+'...----------')\n",
    "    ocean_co2_nonT_recon = model.predict(df_for_recon)\n",
    "    ocean_co2_nonT_recon_list[:,run-1] = ocean_co2_nonT_recon\n",
    "    print(f'Run {run} Complete')\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c063ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now combine all runs into one set\n",
    "ocean_co2_nonT_recon_avg = ocean_co2_nonT_recon_list.mean(axis=1)\n",
    "df_for_recon['ocean_co2_nonT_recon_avg'] = ocean_co2_nonT_recon_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9be0415",
   "metadata": {},
   "outputs": [],
   "source": [
    "xr_ocean_co2_nonT_recon = df_for_recon.ocean_co2_nonT_recon_avg.to_xarray()  #can be computationally costly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9d03ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_var_name = str(target_sel[0]).replace('_nonT','')  #just 'fco2' or 'pco2' for naming things\n",
    "xr_after_ml = xr_for_ml.merge(xr_ocean_co2_nonT_recon, compat='identical')\n",
    "xr_after_ml = xr_after_ml.assign(ocean_co2_recon=xr_after_ml[target_var_name+'_T']+xr_after_ml.ocean_co2_nonT_recon_avg)\n",
    "xr_after_ml = xr_after_ml.transpose('time','ylat','xlon')\n",
    "xr_after_ml = xr_after_ml.rename({'ocean_co2_nonT_recon_avg': target_var_name+'_nonT_recon_avg'\n",
    "                                 ,'ocean_co2_recon': target_var_name+'_recon'\n",
    "                                })\n",
    "#xr_after_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc84c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( str(target_sel[0]).replace('_nonT','') )\n",
    "xr_after_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252b525b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform some clean up before exporting\n",
    "xr_out = xr.Dataset({target_var_name+'_reconstructed':(['time','ylat','xlon'],xr_after_ml[target_var_name+'_recon'].data)\n",
    "                    },coords={'time':(['time'],xr_after_ml.time.values),'ylat':(['ylat'],xr_after_ml.ylat.values),'xlon':(['xlon'],xr_after_ml.xlon.values)})\n",
    "\n",
    "xr_out['fco2_reconstructed'].attrs['description'] = \"fCO2-Residual (XGBoost ensemble mean)\"\n",
    "xr_out.attrs['hyperparameters'] = str(best_params)\n",
    "xr_out.attrs['training_data'] = str(full_dataset_file)\n",
    "xr_out.attrs['training_features'] = str(feature_sel)\n",
    "xr_out.attrs['created'] = str(datetime.datetime.now())\n",
    "xr_out.attrs['methodology'] = \"Bennington et al. (2022), JAMES\"\n",
    "#xr_out.attrs['code'] = \"/home/vbennington/pCO2_Residual/calc_fluxes.ipynb\"\n",
    "xr_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cec7ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output\n",
    "output_netcdf_with_date(xr_out, output_folder+'', 'pCO2_LEAP_XGBoost-'+target_var_name+'-residual-reconstructed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6dfb27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896bc38d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c55d0106",
   "metadata": {},
   "source": [
    "## View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d050da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot maps at different points in time/month to view reconstruction results\n",
    "\n",
    "#varname, plot_data ='pCO2_T' , xr_after_ml.pco2_T.groupby(\"time.month\").mean(\"time\")\n",
    "varname, plot_data ='pCO2_recon' , xr_after_ml[target_var_name+'_recon'].groupby(\"time.month\").mean(\"time\")\n",
    "levelspace = np.linspace(200,560,13)\n",
    "fig,ax = plt.subplots(1,2,figsize=(20,5))\n",
    "x0=ax[0].contourf(xr_after_ml.xlon,xr_after_ml.ylat,plot_data[2,:,:],levels=levelspace,cmap=cm.cm.balance)\n",
    "x0=ax[1].contourf(xr_after_ml.xlon,xr_after_ml.ylat,plot_data[8,:,:],levels=levelspace,cmap=cm.cm.balance)\n",
    "\n",
    "ax[0].set_title(\"Mean Mar\"+ varname); ax[1].set_title(\"Mean Sep \"+varname);\n",
    "ax[0].set_xlabel('Lon'); ax[0].set_ylabel('Lat');\n",
    "ax[1].set_xlabel('Lon'); ax[1].set_ylabel('Lat');\n",
    "plt.colorbar(x0, ax=ax[0]).set_label('uatm');\n",
    "plt.colorbar(x0, ax=ax[1]).set_label('uatm');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1e9768",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_y = xr_after_ml[target_var_name+'_recon'].mean(\"ylat\").mean('xlon') #average global\n",
    "#plt_y = xr_after_ml.pco2_recon.sel(ylat=.5, xlon=.5)  #just at one point\n",
    "fig = plt.figure(figsize=(8,3))\n",
    "plt.plot(ttime,plt_y)\n",
    "plt.grid(True)\n",
    "plt.title('Reconstructed pCO2 (T + nonT)'); plt.xlabel('Time'); plt.ylabel('pCO2 (utam)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff13fbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(3,3))\n",
    "plt.barh(feature_sel, model.feature_importances_)  #note, this is only for the last run\n",
    "plt.xlabel(\"XGBoost pCO2 Feature Importance\")\n",
    "plt.title('v2023 done by ds4114');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leap_co2_v1",
   "language": "python",
   "name": "leap_co2_v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
